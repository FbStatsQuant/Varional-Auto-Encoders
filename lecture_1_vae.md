# Variational Autoencoders

## Introduction

The standard autoencoder learns to compress data into a latent code and reconstruct it. It does this well — but the latent space it produces has no guaranteed structure. Codes are scattered across $\mathbb{R}^r$ in whatever arrangement minimizes reconstruction error, with no control over their distribution. If you sample a random point $z$ from the latent space and decode it, you will often get nonsense, because that point may lie in a region the encoder never mapped anything to during training.

This is not just a generative inconvenience. For anomaly detection, it means you cannot make probabilistic statements about how likely a given input is under the model. For interpolation, it means paths through latent space may pass through dead zones. For any application that requires a *structured*, *regular* latent space, the standard autoencoder is insufficient.

**Variational Autoencoders (VAEs)** fix this by imposing a prior distribution on the latent space. Instead of encoding each input to a single point $z$, the encoder outputs a *distribution* over $z$. Training forces that distribution to stay close to a standard Gaussian. The result is a latent space that is dense, continuous, and probabilistically interpretable — every point in it decodes to something meaningful.

---

## 1. From Autoencoder to Generative Model

### The Problem with Point Encodings

A standard autoencoder encodes $x$ to a single deterministic vector $z = f_\phi(x)$. There is no notion of uncertainty — the encoder is confident about exactly where $x$ lives in latent space.

This creates two problems:

1. **Holes**: regions of latent space that correspond to no training point, where the decoder produces garbage.
2. **No sampling**: to generate new data you need to know *where* to sample $z$. Without a defined distribution over $z$, you have no principled answer.

### The Generative Perspective

A VAE frames the problem probabilistically. We assume data is generated by a two-step process:

$$z \sim p(z) = \mathcal{N}(0, I)$$
$$x \sim p_\theta(x \mid z)$$

A latent code $z$ is first drawn from a standard Gaussian prior. The data $x$ is then generated by a decoder parameterized by $\theta$.

Learning means finding $\theta$ such that the marginal likelihood $p_\theta(x) = \int p_\theta(x \mid z)\, p(z)\, dz$ is high for training data. This integral is intractable — we cannot compute it directly. The VAE resolves this with **variational inference**.

---

## 2. The ELBO

### Variational Inference

Since $p_\theta(x)$ is intractable, we introduce an approximate posterior $q_\phi(z \mid x)$ — the encoder — that approximates the true posterior $p_\theta(z \mid x)$.

We want $q_\phi(z \mid x)$ to be close to $p_\theta(z \mid x)$. Minimizing their KL divergence leads to maximizing the **Evidence Lower BOund (ELBO)**:

$$\log p_\theta(x) \geq \mathbb{E}_{q_\phi(z \mid x)}\left[\log p_\theta(x \mid z)\right] - D_{\mathrm{KL}}\left(q_\phi(z \mid x) \,\|\, p(z)\right)$$

This is the objective the VAE maximizes. Its two terms have a clear interpretation:

| Term | Role |
|---|---|
| $\mathbb{E}[\log p_\theta(x \mid z)]$ | **Reconstruction**: how well does the decoder recover $x$ from $z$? |
| $D_{\mathrm{KL}}(q_\phi \,\|\, p)$ | **Regularization**: how close is the encoder's distribution to the prior $\mathcal{N}(0, I)$? |

In practice we minimize the negative ELBO, i.e.:

$$\mathcal{L}(\phi, \theta) = \underbrace{\mathbb{E}_{q_\phi(z \mid x)}\left[\|x - \hat{x}\|^2\right]}_{\text{reconstruction loss}} + \underbrace{D_{\mathrm{KL}}\left(q_\phi(z \mid x) \,\|\, \mathcal{N}(0, I)\right)}_{\text{KL divergence}}$$

### Closed-Form KL

The encoder outputs a diagonal Gaussian: $q_\phi(z \mid x) = \mathcal{N}(\mu, \mathrm{diag}(\sigma^2))$. With a standard Gaussian prior, the KL has a closed form:

$$D_{\mathrm{KL}} = -\frac{1}{2} \sum_{j=1}^r \left(1 + \log \sigma_j^2 - \mu_j^2 - \sigma_j^2\right)$$

No numerical integration required. The encoder only needs to output $\mu$ and $\log \sigma^2$, and the KL is computed analytically.

---

## 3. The Reparameterization Trick

### The Problem

To train by backpropagation, we need to differentiate through the sampling step $z \sim q_\phi(z \mid x) = \mathcal{N}(\mu, \sigma^2 I)$. Sampling is a stochastic operation — gradients cannot flow through it directly.

### The Solution

Reparameterize the sample as a deterministic function of $\mu$, $\sigma$, and a separate noise variable $\varepsilon$:

$$z = \mu + \sigma \odot \varepsilon, \qquad \varepsilon \sim \mathcal{N}(0, I)$$

Now the randomness lives entirely in $\varepsilon$, which has no parameters. The gradient flows through $\mu$ and $\sigma$ normally. The network learns *where* to center the distribution and *how wide* to make it — the sampling itself is just adding fixed noise.

$$\frac{\partial z}{\partial \mu} = 1, \qquad \frac{\partial z}{\partial \sigma} = \varepsilon$$

This is the reparameterization trick. Without it, VAEs cannot be trained end-to-end with gradient descent.

---

## 4. Architecture

### Encoder

The encoder maps input $x$ to the parameters of a Gaussian distribution, not to a single point:

$$h = \mathrm{EncoderNet}(x)$$
$$\mu = W_\mu h + b_\mu$$
$$\log \sigma^2 = W_{\log\sigma^2} h + b_{\log\sigma^2}$$

Two separate linear heads produce $\mu \in \mathbb{R}^r$ and $\log \sigma^2 \in \mathbb{R}^r$. We output $\log \sigma^2$ rather than $\sigma^2$ to keep the parameterization unconstrained — the network can output any real number, and $\sigma^2 = e^{\log \sigma^2}$ is always positive.

### Decoder

The decoder is identical to a standard autoencoder decoder. It takes a sample $z$ and produces a reconstruction $\hat{x}$:

$$\hat{x} = \mathrm{DecoderNet}(z)$$

### Comparison to Standard Autoencoder

| | Autoencoder | VAE |
|---|---|---|
| Encoder output | Point $z \in \mathbb{R}^r$ | Distribution $(\mu, \log\sigma^2) \in \mathbb{R}^{2r}$ |
| Latent sample | $z = f_\phi(x)$ (deterministic) | $z = \mu + \sigma \odot \varepsilon$ (stochastic) |
| Loss | Reconstruction only | Reconstruction + KL divergence |
| Latent space | Unstructured | Regularized toward $\mathcal{N}(0, I)$ |
| Generation | No principled sampling | Sample $z \sim \mathcal{N}(0, I)$, decode |

### Implementation in PyTorch

```python
import torch
import torch.nn as nn
import torch.nn.functional as F


class VAE(nn.Module):
    def __init__(self, input_dim=784, latent_dim=2):
        super().__init__()

        # Encoder — shared trunk
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 128),
            nn.ReLU()
        )
        # Two separate heads: one for mu, one for log_var
        self.mu      = nn.Linear(128, latent_dim)
        self.log_var = nn.Linear(128, latent_dim)

        # Decoder
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 256),
            nn.ReLU(),
            nn.Linear(256, input_dim),
            nn.Sigmoid()   # use identity if input is standardized
        )

    def reparameterize(self, mu, log_var):
        std = torch.exp(0.5 * log_var)   # sigma = exp(0.5 * log_var)
        eps = torch.randn_like(std)       # eps ~ N(0, I)
        return mu + eps * std             # z = mu + sigma * eps

    def forward(self, x):
        h       = self.encoder(x)
        mu      = self.mu(h)
        log_var = self.log_var(h)
        z       = self.reparameterize(mu, log_var)
        x_recon = self.decoder(z)
        return x_recon, mu, log_var


def vae_loss(x_recon, x, mu, log_var):
    # Reconstruction: how well did we recover x?
    recon_loss = F.mse_loss(x_recon, x, reduction='sum')
    # KL: how far is q(z|x) from N(0, I)?
    kl_loss    = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())
    return recon_loss + kl_loss


# Training loop
model     = VAE(input_dim=784, latent_dim=2).to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

for X, _ in train_loader:
    X       = X.view(X.size(0), -1).to(device)
    x_recon, mu, log_var = model(X)
    loss    = vae_loss(x_recon, X, mu, log_var)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```

---

## 5. The KL Term as Regularization

The KL divergence term does two things that matter in practice.

**It prevents posterior collapse into point estimates.** Without it, the encoder would learn to set $\sigma \to 0$ for every input, collapsing back into a standard autoencoder. The KL term penalizes this — zero variance means $\log \sigma^2 \to -\infty$, which sends the KL to infinity.

**It fills the latent space.** By pulling every posterior $q_\phi(z \mid x)$ toward $\mathcal{N}(0, I)$, it ensures that the entire unit Gaussian region of latent space is covered by training data. Decoding a random sample $z \sim \mathcal{N}(0, I)$ will land in a region the decoder has been trained on — unlike the standard autoencoder, where most of $\mathbb{R}^r$ is empty.

The tradeoff is that stronger KL regularization forces the encoder to compress more aggressively, which can hurt reconstruction quality. The $\beta$-VAE addresses this explicitly.

---

## 6. The β-VAE

### Motivation

The standard VAE weights reconstruction and KL equally. In practice, their scales differ — MSE over high-dimensional inputs is large, while KL over a small latent space is small. The two terms do not compete equally without reweighting.

The **β-VAE** introduces a scalar $\beta \geq 1$ that upweights the KL term:

$$\mathcal{L}_\beta(\phi, \theta) = \mathbb{E}\left[\|x - \hat{x}\|^2\right] + \beta \cdot D_{\mathrm{KL}}\left(q_\phi(z \mid x) \,\|\, \mathcal{N}(0, I)\right)$$

### Effect of β

| $\beta$ | Effect |
|---|---|
| $\beta = 0$ | Standard autoencoder — no KL regularization |
| $\beta = 1$ | Standard VAE |
| $\beta > 1$ | Stronger regularization — more disentangled latent space, potentially worse reconstruction |

Higher $\beta$ encourages the model to use each latent dimension independently to encode a single factor of variation — this is the **disentanglement** effect. In practice $\beta$ is a hyperparameter tuned to the task.

For anomaly detection, higher $\beta$ tends to help because the KL term amplifies the anomaly signal: fraudulent inputs are forced into unusual regions of the latent space, making them easier to separate from normal inputs by their anomaly score.

---

## 7. VAEs for Anomaly Detection

### The Core Idea

Train the VAE exclusively on **normal** data. After training, the model has learned to reconstruct normal inputs well and to map them to the region near the origin in latent space.

When a novel (potentially anomalous) input is presented:
- The encoder maps it to a region of latent space that was not covered during training.
- The decoder, which has only learned normal patterns, fails to reconstruct it.
- Both the reconstruction error and the KL divergence are high.

An **anomaly score** is computed per sample as the sum of both terms:

$$s(x) = \underbrace{\|x - \hat{x}\|^2}_{\text{reconstruction loss}} + \underbrace{D_{\mathrm{KL}}(q_\phi(z \mid x) \,\|\, \mathcal{N}(0, I))}_{\text{KL loss}}$$

This is simply the per-sample ELBO loss. A high score indicates the input is anomalous.

```python
def get_anomaly_scores(model, data):
    model.eval()
    with torch.no_grad():
        x_recon, mu, log_var = model(data)
        recon = F.mse_loss(x_recon, data, reduction='none').sum(dim=1)
        kl    = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp(), dim=1)
        return (recon + kl).numpy()
```

### Threshold Selection

A threshold $\tau$ is set at a high percentile (e.g., 95th or 99th) of the anomaly scores on held-out **normal** data:

$$\tau = \mathrm{percentile}_{99}\left(\{s(x_i) : x_i \in \text{normal test set}\}\right)$$

Any input with $s(x) > \tau$ is flagged as anomalous. The percentile controls the precision-recall tradeoff:

| Threshold | Recall | Precision | False alarms |
|---|---|---|---|
| 90th pct | High | Low | Many |
| 99th pct | Lower | High | Few |

The right choice depends on the cost of missing fraud versus the cost of investigating false alarms.

### Why VAE Outperforms Standard Autoencoder for Anomaly Detection

A standard autoencoder may assign low reconstruction error to anomalies if they accidentally map to a region of latent space that decodes similarly to normal data. The KL term in the VAE prevents this: anomalies produce latent distributions that are far from $\mathcal{N}(0, I)$, contributing a high KL loss even when reconstruction error is moderate. The anomaly score is therefore more robust.

---

## 8. Strengths and Weaknesses

### Strengths

**Structured latent space**: The KL term enforces a prior over $z$, making the latent space dense and continuous. Every point near the origin decodes to something meaningful — enabling generation, interpolation, and principled anomaly scoring.

**Principled generative model**: The VAE defines an explicit (approximate) likelihood $p_\theta(x)$. This probabilistic foundation makes it suitable for tasks that require uncertainty quantification, unlike the standard autoencoder.

**Unsupervised anomaly detection**: By training only on normal data, the VAE learns what "normal" looks like. Anomalies are detected without ever seeing a labeled anomaly — the per-sample ELBO naturally separates normal from anomalous inputs.

**Reparameterization enables end-to-end training**: By decoupling the stochasticity from the parameters, the reparameterization trick allows standard backpropagation through the sampling step — no special training procedure required.

### Weaknesses

**Blurry reconstructions**: The reconstruction loss averages over all plausible decodings of $z$, producing blurry outputs on image data. This is a known limitation of MSE-based VAEs. Alternatives like perceptual loss or adversarial training (VAE-GAN) address this.

**Posterior collapse**: With strong KL weighting or deep decoders, the model can ignore $z$ entirely — the decoder learns to reconstruct from bias alone, and the encoder outputs $\mathcal{N}(0, I)$ regardless of $x$. This is called posterior collapse and is an active research problem.

**ELBO is a lower bound, not the true likelihood**: The VAE does not maximize $\log p_\theta(x)$ directly. The gap between the ELBO and the true likelihood depends on how well $q_\phi(z \mid x)$ approximates the true posterior — a diagonal Gaussian may be a poor approximation for complex distributions.

**Sensitivity to $\beta$**: In the β-VAE, $\beta$ controls a fundamental tradeoff between reconstruction quality and latent space regularity. There is no universally correct value; it must be tuned per task and dataset.

---

## 9. Summary

1. **The standard autoencoder lacks a structured latent space** — codes are scattered arbitrarily, making generation and anomaly scoring ill-defined. The VAE fixes this by treating the latent space as a probability distribution.

2. **The VAE encoder outputs a distribution** $q_\phi(z \mid x) = \mathcal{N}(\mu, \sigma^2 I)$, not a single point. Two separate linear heads produce $\mu$ and $\log \sigma^2$.

3. **Training maximizes the ELBO**: reconstruction loss (how well we recover $x$) minus KL divergence (how far the encoder distribution is from the prior $\mathcal{N}(0, I)$). The KL has a closed form for Gaussian distributions.

4. **The reparameterization trick** $z = \mu + \sigma \odot \varepsilon$ decouples the stochastic sampling from the parameters, allowing gradients to flow through $z$ and enabling end-to-end training.

5. **The β-VAE** upweights the KL term by $\beta > 1$, encouraging a more regularized and disentangled latent space at the cost of reconstruction quality. Higher $\beta$ generally improves anomaly detection performance.

6. **For anomaly detection**, the per-sample ELBO (reconstruction loss + KL) serves as an anomaly score. Training on normal data only, the model assigns high scores to anomalous inputs — which the encoder maps to unusual latent regions and the decoder fails to reconstruct.

---

## References

- Kingma, D. P., & Welling, M. (2014). Auto-encoding variational Bayes. *ICLR*.
- Higgins, I., et al. (2017). β-VAE: Learning basic visual concepts with a constrained variational framework. *ICLR*.
- An, J., & Cho, S. (2015). Variational autoencoder based anomaly detection using reconstruction probability. *SNU Data Mining Center*.
- Rezende, D. J., Mohamed, S., & Wierstra, D. (2014). Stochastic backpropagation and approximate inference in deep generative models. *ICML*.
- Doersch, C. (2016). Tutorial on variational autoencoders. *arXiv:1606.05908*.

---

